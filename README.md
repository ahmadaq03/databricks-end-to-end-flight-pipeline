# databricks-end-to-end-flight-pipeline
End-to-End Databricks Lakehouse Flight Pipeline âœˆï¸

A complete data engineering pipeline built on the Databricks Lakehouse, demonstrating real-world ingestion and transformation techniques using PySpark, Autoloader, Delta Live Tables, and SCD logic.

ðŸ“Œ Features

Incremental ingestion with Databricks Autoloader

Structured Streaming for new flight/booking data

Bronze â†’ Silver â†’ Gold Medallion Architecture

Delta Live Tables (DLT) for declarative pipelines

Automated Slowly Changing Dimensions (SCD Type 2)

Creation of fact and dimension tables for analytics

Unity Catalog organization

data/         -> Raw + incremental datasets
notebooks/    -> Databricks notebook (.dbc)
images/       -> Architecture diagram

ðŸ§± Architecture



ðŸš€ How to Run

Import the .dbc notebook into Databricks

Upload CSVs into a UC volume

Configure Autoloader paths

Run Bronze â†’ Silver â†’ Gold logic

Execute DLT pipeline

Validate fact/dim outputs

ðŸ”§ Tech Stack

Databricks

PySpark

Delta Lake

Autoloader

Delta Live Tables

Unity Catalog
